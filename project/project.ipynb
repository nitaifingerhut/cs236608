{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1) Mission statment\n",
    "2) Topics-Dynamic User Model\\\n",
    "    2.1) Why it does not represent the phanomena we deal with\\\n",
    "    2.2) Preferences change over time\n",
    "3) Topics-Dynamic-Reverse User Model\\\n",
    "Preferences change over time\n",
    "4) Baseline - Vanilla AutoRec\\\n",
    "    5.1) Multiple decision rules\\\n",
    "    5.2) MRSE\\\n",
    "    5.3) ARRI\\\n",
    "    5.4) Model Convergence\n",
    "5) AutoRec with ratings history\\\n",
    "    6.1) Different temporal window size\\\n",
    "    6.2) Different initialization user-item-window tensor\\\n",
    "    6.3) MRSE\\\n",
    "    6.4) ARRI\\\n",
    "    6.5) Model Convergence\n",
    "6) AutoRec with ratings history & recommendations history\\\n",
    "    7.1) Different temporal window size\\\n",
    "    7.2) Different initialization user-item-window tensor\\\n",
    "    7.3) MRSE\\\n",
    "    7.4) ARRI\\\n",
    "    7.5) Model Convergence\n",
    "7) Topic-Dynamic-Reverse Recommender-Agnostic\\\n",
    "Breaking the recommender-environment feedback loop\n",
    "8) Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1: Mission Statment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommending items from a fixed-size set of items to a fixed-szie set of users is an open challenge for the following reasons (partial list): \n",
    "1) i.i.d principle violation:\\\n",
    "One of the basic assumptions in training a 'classic' predictive model is independence & identically distribution of the training data.\\\n",
    "As a consequnce, one assumes that all data points presented to the model during training are drawn from a time-invariant distribution function.\\\n",
    "But in Rec Systems, this is not hold. The users change their behaviors (their underlyning distributions) as a response to the recommender decisions.\\\n",
    "This forces the recommender to be strategic in the face of uncertainity, which is quite difficult during training, when the recommender is yet uncapable in term of predictive performance. \n",
    "2) Even regardless to the feedback loop between the recommender & the users, still the user model changes frequently over time during training.\\\n",
    "Since the recommender model (at least the vanilla version) does not have a ense of time, it is hardly capable learning the prefernces changing function, even when it is user-invariant, time-invariant and changes slow enough. \n",
    "3) We hypothesize that if a model would be capable to learn the users underlying change function, it could use this information for better predictions, which in turn would lead to better recommendations.\n",
    "4) We examined various recommender models that were trained on a tensor of obeserved ratings over last k iterations rather than a single frameshot ratings matrix in the vanilla AutoRec version.\n",
    "5) Furthermore, wed hypothesized that if we will feed the model with its previous decisions (another temporal window of recommendations), it will be able to mitigate the phanomena presented in 1).\n",
    "TL;DR it didn't work though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2: Topics-Dynamic User-Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are countless user models, aka, ways that the user can change their behavior over time.\\\n",
    "We choose to keep it simple, and stay close as possible to the Topics-Dynamic User-Model we saw in class.\\\n",
    "In this User-Model, all items are grouped into topics, and users prefer topics, rather than an individual items.\\\n",
    "The preferences of user $u$ for items $i$ of topic $k$\n",
    "is initialized as $\\pi(u, k) \\sim U(0.5, 5.5)$,\\\n",
    "while the topic, $k$ of item $i$ is chosen randomly from the set of all topics.\\\n",
    "When user $u$ is recommended item $i$ it will rate the item as $r_t(u, i)=clip(\\pi(u, k)+\\epsilon)$ where $\\epsilon \\sim N(0,\\sigma^2)$ represents exogenous noise not,\n",
    "modeled by the simulation, and clip truncates values to be between $1$ and $5$.\\\n",
    "if at round $t$ user $u$ is recommended an item of topic $k$,\\\n",
    "then at the next round her preferences are updated to be:\\\n",
    "$\\pi_{t+1}(u, k) ← clip(\\pi_t(u, k) + a),$\\\n",
    "$\\pi_{t+1}(u, k') ← clip(\\pi_t(u, k') - \\frac{a}{K-1}) \\quad \\forall k' \\neq k$\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2.1: Why it does not represent the phanomena we are intrested**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one could understand from the equation above, this User Model makes the users to like the recomended items.\\\n",
    "In other words, even if initially the users don't favor the topic of the recommended items, quickly they 'change thier minds' and start to like it.\\\n",
    "We think that this causes the ARRI for the dynamic enviroment to raise above the ARRI for the static environment, even without expoloration et all, whether the recommender has succeded to predict the preferences well or not.\\\n",
    "We want to see what happens when the recommendations **affect badly** on the user welfare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TOPICS_DYNAMIC_AUTO_ARRI](project_artifacts/env_d_eg_0_0/ARRI.png)\\\n",
    "Fig 1: Autorec with Topics-Dynamic env with change coeeficient 0 (Topics-Static) & 1 (Topics-Dynamic with slow drift).\\\n",
    "While the ARRI for the static version is around 4.0 at best, the ARRI for the dynamic version is around 4.75.\\\n",
    "Averaged over 10 repeats: the center line is the average, and the vertical lines are error bars (std). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 2: User preferences change over time (and don't change, for Topics-Static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3: Topics-Dynamic Reverse User-Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a scenario which is intersting, we modified the User Model source code, and created a 'rebelling' version of the user.\\\n",
    "That is, a user which tends to dislike whatever the recommender recommeds him.\\\n",
    "Thed user preferences change equations are similarf to the previous but with a little change:\\\n",
    "$\\pi_{t+1}(u, k) ← clip(\\pi_t(u, k) - a),$\\\n",
    "$\\pi_{t+1}(u, k') ← clip(\\pi_t(u, k') + \\frac{a}{K-1}) \\quad \\forall k' \\neq k$\\\n",
    "**We would consider this as a baseline for our next experiments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![REVERSE_TOPICS_DYNAMIC_AUTO_ARRI](project_artifacts/env_dr_eg_0_0/ARRI.png)\\\n",
    "Fig 3: Reverse-Topics-Dynamic.\\\n",
    "ARRI for the environment with the preferences change now is lower than the static version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![REVERSE_TOPICS_DYNAMIC_AUTO_EPS_025_ARRI](project_artifacts/env_dr_eg_0_25/ARRI.png)\\\n",
    "Fig 4: Reverse-Topics-Dynamic. Autorec recommender with Epsilon Greedy decision rule (0.25)\\\n",
    "ARRI is not changed much. It's not a matter of exploration, but fundumental with the dynamics of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 4: User preferences change over time for Reverse-Topics-Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.1 - Intermediate observation/hypothesis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARRI for the Reverse-Topics-Dynamic is lower than the static version, and roughly fluctuate around the default rating of the Model's users-items-ratings matrix, which is 3.\\\n",
    "We hypothesis that the recommender model do not succeed to learn the preferences **change function** due to a lack of temporal information. Note that the function is time-invariant, user-invariant.\\\n",
    "We think that if we would provide the recommender model some **temporal information** about the oveserved ratings arrived from the users, it would be able to learn the preferences drift, and recommend items accordignly.\\\n",
    "For example, a better prediction using the temporal data, combined with other-than greedy decision rule (epsilon greedy for instance), can help the recommender recommend better items, in the face of lower uncertanity in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_0/RMSE.png)\\\n",
    "Fig 6: Reverse-Topics-Dynamic RMSE.\\\n",
    "While the model predictive error gets lower for the static version as the simulation continous, predictive error for the reverse-dynamic version gets higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 5 - Vannila Autorec:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we evaluated the regular Autorec recommender (https://arxiv.org/abs/2007.07224) with our new user model, different epsilon greedy values and averaged over 10 independent experiments.\\\n",
    "In short, Vanilla Autorec failed to increase its ARRI over the reverse-dynamic-env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 5.1 - Different Epsilon Greedy values:**\n",
    "\n",
    "To show that the problem is not related to the decision rule, we evaluated the follwoing values: 0.0, 0.25, 0.5.\\\n",
    "TL;DR: this is not the problem.\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_0/ARRI.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_25/ARRI.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_5/ARRI.png)\\\n",
    "Fig 7: ARRI with epsilon greedy 0, 0.25, 0.5 (left to right).\n",
    "While as expected, more exploration adds more variability to the results (higher std), it does not improve the overall expected ARRI.\n",
    "\n",
    "\n",
    "**Section 5.2 - Predictive Errors:**\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_0/RMSE.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_25/RMSE.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_5/RMSE.png)\\\n",
    "Fig 8: RMSE with epsilon greedy 0, 0.25, 0.5 (left to right).\\\n",
    "In all cases, in contrast to the static version of the env, RMSE (predictive error) for the reverse dynamic env gets higher. \n",
    "\n",
    "**Section 5.3 - Model Convergence:**\n",
    "\n",
    "To aassure that it is not a problem of the model's capacity to learn the users-items-ratings matrix, or any other possible failures of the AutoEncoder Neural net model to learn, we show the training loss of the model, for the last iteration of the simulation.\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_0/REC_LAST_STEP_LOSSES.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_25/REC_LAST_STEP_LOSSES.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_AUTO_RMSE](project_artifacts/env_dr_eg_0_5/REC_LAST_STEP_LOSSES.png)\\\n",
    "Fig 9: Last iteration of the simulation training loss.\\\n",
    "These plots show that the model is capable to learn the users-items-ratings matrix (from capacity perspective or optimization algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 7 - Topic-Dynamic-Reverse Recommender-Agnostic:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wondered wether the negative impact on the ARRI is due to the poor predictive performance, or is it because some negative effect of the recommender-environment feedback loop.\\\n",
    "As we spoke in class, the recommender decisions affects the data distribution, breaking the i.i.d assumption in the basis of all predictive models, hence can causes weird behaviors.\\\n",
    "To check and isolate this effect, we modified the Topics-Dynamics source code, to change randomly regardless the recommender decisions.\\\n",
    "So we still have a time-invariant, user-invariant preferences change function, but now it is also recommender-invariant as well.\\\n",
    "For each user, at reset, a single topic is chosen randomly. Then, each time this user receive a recommendation, regardless the recommended item, the chosen topic preference is being reduced, and the rest are being increased (like regular Reverse-Topic-dynamic).\n",
    "We repeated the previous experiments with the new environment to see if the source for the reduced performence is a lack of temporal information (as previously assumed), or the feedback loop itself.\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_ARRI](project_artifacts/env_drra_eg_0_0/ARRI.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_ARRI](project_artifacts/env_drra_eg_0_25/ARRI.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_ARRI](project_artifacts/env_drra_eg_0_5/ARRI.png)\\\n",
    "Fig 10: Reverse-Topic-Dynamic Recommendation-Agnostic ARRI.\\\n",
    "We observe that the recommender succeed (kind of) to learn the preferences change function. ARRI for the dynamic version converges with the ARRI for the static version.\n",
    "\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_RMSE](project_artifacts/env_drra_eg_0_0/RMSE.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_RMSE](project_artifacts/env_drra_eg_0_25/RMSE.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_RMSE](project_artifacts/env_drra_eg_0_5/RMSE.png)\\\n",
    "Fig 11: Reverse-Topic-Dynamic Recommendation-Agnostic RMSE.\\\n",
    "Also from predictive error perspective: RMSE drops as the simulation continous.\\\n",
    "This in contrast to the equivalent environment with recommender-environment feedback loop (see section 5)\n",
    "\n",
    "\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_LOSS](project_artifacts/env_drra_eg_0_0/REC_LAST_STEP_LOSSES.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_LOSS](project_artifacts/env_drra_eg_0_25/REC_LAST_STEP_LOSSES.png)\n",
    "![REVERSE_TOPICS_DYNAMIC_REC_AGNO_AUTO_LOSS](project_artifacts/env_drra_eg_0_5/REC_LAST_STEP_LOSSES.png)\\\n",
    "Fig 12: Reverse-Topic-Dynamic Recommendation-Agnostic Last iteration loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c81bb6c5d07bc74241c85ef62b264aeeba785eadb258e083c9c642a1ca48f83"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('cs236608')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
